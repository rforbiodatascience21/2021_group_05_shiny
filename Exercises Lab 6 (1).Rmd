---
title: "Exercises Lab 6 - Modelling in the Tidyverse"
author: "Lucille Augusta Green"
date: "8 Feb 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE}
library("tidyverse")
library("fs")
library("patchwork")
library("broom")
```

# Loading the Data 
```{r}
load(file = "Data/gravier.RData")
#Examine the data 
str(gravier)
```
# Wrangling the data set 

```{r}
gravier_data <- gravier$x %>% as.tibble() %>% 
  mutate(outcome = as.integer(gravier$y)-1)
gravier_data
```

```{r}
gravier_data %>%  group_by(outcome) %>% summarise(n = n())
```
## Modelling
* Q1: What are the coefficients for the intercept and your gene? * 

- use summary or tidy to get summary statistics of the model. 

```{r}
gravier_data %>% glm(outcome ~ g1int570, 
                    data = .,
                    family = binomial(link = "logit")) %>% summary()
```

```{r}
gravier_data_long <- gravier_data %>% 
  pivot_longer(cols = -outcome, names_to = "gene", values_to = "log2_expr_level")

gravier_data_long
```

```{r}
gravier_data_long_nested <- gravier_data_long %>%  group_by(gene) %>% nest() %>%  ungroup()

gravier_data_long_nested
```

```{r}
set.seed(1234)
gravier_data_long_nested <- gravier_data_long_nested %>%  sample_n(100) # takes 100 random genes in the data set
gravier_data_long_nested
```
```{r}
# Creating a logistic model for each of the random chosen genes. 
# The outcome as a function of the log2_expr_level
gravier_data_long_nested <- gravier_data_long_nested %>%  mutate(mdl = map(data, ~ glm(outcome ~ log2_expr_level, 
                               data = .x, 
                               family = binomial(link = "logit"))))

gravier_data_long_nested
```
 
 Ok, now we have model for each of our 100 genes. Let us use the broom-package to extract some information from each of the models

Hint: broom has a very useful function, which helps “cleaning up” model objects. Also, you may want to include conf.int = TRUE in your map()-call, this will add confidence levels. 

```{r}
gravier_data_long_nested <- gravier_data_long_nested %>%  
  mutate(mdl_tidy = map(mdl, tidy, conf.int = TRUE)) %>% 
  unnest(mdl_tidy) # "ungrouping" the statistic-variables, so they are shown in the overall table. 

gravier_data_long_nested
# The tidy function gives 5 variables for each of the random 100 models in the data set
# The variables are : Term, estimate, sd. error, statistic, p-value
```
Now, we are only interested in the terms for the genes, so remove the (Intercept)-rows:

Hint: Check, that your dimensions should go from 200 to 100, corresponding to the 100 random genes you selected for further analysis!

```{r}
gravier_data_long_nested <- gravier_data_long_nested %>% filter(term == "log2_expr_level")
gravier_data_long_nested
```

Add an indicator variable, denoing if a given term for a given gene is significant i.e. p<0.05

```{r}
gravier_data_long_nested <- gravier_data_long_nested %>%
  mutate(identified_as = case_when( p.value < 0.05 ~ "Significant",
                                               TRUE ~ "Non-significant"))

gravier_data_long_nested
```
## Visualise Associations
A Useful way of illustrating p-values for gene association is the so-called Manhattan-plot. For this we need to calculate the negative log10 of the p-values, so do that like so:

```{r}
gravier_data_long_nested <- gravier_data_long_nested %>%
  mutate(neg_log10_p = -log10(p.value))

gravier_data_long_nested
```
Now, using your data visualisations skills, re-create this Manhattan-plot

```{r}
gravier_data_long_nested %>% 
  ggplot(mapping = aes(x = fct_reorder(gene, neg_log10_p, .desc = TRUE), 
                       y = neg_log10_p,
                       color = identified_as)) +
  geom_point() + 
  geom_hline(yintercept = - log10(0.05), linetype = "dashed") +
  theme_classic(base_size = 8, base_family = "Avenir") +
  theme(legend.position = "bottom", 
        axis.text.x = element_text(angle = 45 , vjust = 1, hjust = 1)) +
  labs( x = "Gene", y = "Minus log10(p)")
```
A more interesting visualisation is a confidence interval plot with effect directions 

```{r fig.align='center', fig.width=8, fig.height=8}
gravier_data_long_nested %>% 
  ggplot(mapping = aes(x = estimate, y = fct_reorder(gene, estimate, .desc = TRUE),
                       color = identified_as)) + 
  geom_point() + 
  geom_vline(xintercept = 0, linetype = "dashed" ) + 
  geom_errorbarh(aes(xmin = conf.low,
                     xmax = conf.high,
                     height = 0.1)) +
  theme_classic(base_size = 8, base_family = "Avenir") +
  theme(legend.position = "bottom" ) +
  labs( x = "Estimate", y = "Gene", caption = "The 111 patients with no event after diagnosis ....")
```

do the same plot, but now only including genes with a significant association
```{r fig.align='center', fig.width=8, fig.height=8}
gravier_data_long_nested %>% 
  filter(identified_as == "Significant") %>% 
  ggplot(mapping = aes(x = estimate, y = fct_reorder(gene, estimate, .desc = TRUE))) + 
  geom_point() + 
  geom_vline(xintercept = 0, linetype = "dashed" ) + 
  geom_errorbarh(aes(xmin = conf.low,
                     xmax = conf.high,
                     height = 0.1)) +
  theme_classic(base_size = 8, base_family = "Avenir") +
  theme(legend.position = "bottom" ) +
  labs( x = "Estimate", y = "Gene", caption = "The 111 patients with no event after diagnosis ....")
```
Note, we have not taken multiple testing into account. If you have the time, you could redo the above with adjust p-values. See ?p.adjust

## PCA
```{r}
library("broom")
library("patchwork")
```


Now, let us play around with around with a little PCA, first add and run this little chunk, allowing us to continue our work on the 100 genes we previously selected:
```{r}
gravier_data_wide = gravier_data %>%
  select(outcome, pull(gravier_data_long_nested, gene))
gravier_data_wide
```
In general, when performing PCA, we’ll want to do (at least) three things:

- Look at the data in PC coordinates.
- Look at the rotation matrix.
- Look at the variance explained by each PC.

We start by running the PCA and storing the result in a variable pca_fit. There are two issues to consider here. First, the prcomp() function can only deal with numeric columns, so we need to remove all non-numeric columns from the data. This is straightforward using the where(is.numeric) tidyselect construct. Second, we normally want to scale the data values to unit variance before PCA. We do so by using the argument scale = TRUE in prcomp().

```{r}
gravier_data_wide <- gravier_data_wide %>% 
  mutate(outcome = as_factor(outcome))

pca_fit <- gravier_data_wide %>%  
  select(where(is.numeric)) %>% # retain only numeric columns
  prcomp(scale = TRUE) # do PCA on scaled data
```

# Plotting the data in PC coordinates

```{r}
pca_fit %>%
  augment(gravier_data_wide) %>% # add original dataset back in
  ggplot(aes(.fittedPC1, .fittedPC2, color = outcome)) + 
  geom_point(size = 1.5) + 
  theme_minimal() 
# Can't change the colors to take the two discrete values 0 and 1 ...
# scale_color_manual( values = c("0" = "#D55E00", "1" = "#0072B2")) + 
```
# PLotting the rotation matrix

```{r}
# extract rotation matrix
pca_fit %>%
  tidy(matrix = "rotation")
```
```{r}
# define arrow style for plotting
arrow_style <- arrow(
  angle = 20, ends = "first", type = "closed", length = grid::unit(8, "pt")
)

# plot rotation matrix
pca_fit %>%
  tidy(matrix = "rotation") %>%
  pivot_wider(names_from = "PC", names_prefix = "PC", values_from = "value") %>%
  ggplot(aes(PC1, PC2)) +
  geom_segment(xend = 0, yend = 0, arrow = arrow_style) +
  geom_text(
    aes(label = column),
    hjust = 1, nudge_x = -0.02, 
    color = "#904C2F"
  ) +
  coord_fixed() + # fix aspect ratio to 1:1
  theme_minimal_grid(12)
```

# Plotting the variance explained by each PC

```{r}
pca_fit %>%
  tidy(matrix = "eigenvalues")
```
```{r}
pca_fit %>%
  tidy(matrix = "eigenvalues") %>% filter(percent > 0.027) %>% 
  ggplot(aes(PC, percent)) +
  geom_col(fill = "#56B4E9", alpha = 0.8) +
  scale_x_continuous(breaks = 1:9) +
  scale_y_continuous(
    labels = scales::percent_format(),
    expand = expansion(mult = c(0, 0.01))
  ) +
  theme_minimal_hgrid(12)
```

## K means 

```{r}
kclust <- gravier_data_wide %>% kmeans(centers = 50)
```

```{r}
augment(kclust, gravier_data_wide)
```
```{r}
tidy(kclust) #The tidy() function summarizes on a per-cluster level:

```
```{r}
glance(kclust) # the glance() function extracts a single-row summary

```

Let’s say we want to explore the effect of different choices of k, from 1 to 9, on this clustering. First cluster the data 9 times, each using a different value of k, then create columns containing the tidied, glanced and augmented data

```{r}
kclusts <- 
  tibble(k = c(1,5,10,20,50,150)) %>%
  mutate(
    kclust = map(k, ~kmeans(gravier_data_wide, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, gravier_data_wide)
  )
```
We can turn these into three separate data sets each representing a different type of data: using tidy(), using augment(), and using glance(). Each of these goes into a separate data set as they represent different types of data

```{r}
clusters <- 
  kclusts %>%
  unnest(cols = c(tidied))

assignments <- 
  kclusts %>% 
  unnest(cols = c(augmented))

clusterings <- 
  kclusts %>%
  unnest(cols = c(glanced))
```

Now we can plot the original points using the data from augment(), with each point colored according to the predicted cluster.

```{r}
p1 <- 
  ggplot(assignments, aes(x = g8H05, y = g4F03)) +
  geom_point(aes(color = .cluster), alpha = 0.8) + 
  facet_wrap(~ k)
p1
```

